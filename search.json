[{"title":"Peaceful Rooks","path":"/2024/08/22/peaceful-rooks/","content":"Peaceful RooksYou have a chess board, which is a 8 x 8 board, and you want to have 8 peaceful rooks live in harmony together. Then, because you are a mathematician (and a masochist), you want to find out how many different positions we can place these rooks so that they are all peaceful. A peaceful board state is where none of the pieces can attack another piece (you can even say that starting out a chess game the board is peaceful). Here is an example of a peaceful board. As you can see, none of the rooks are attacking each other. It is your job to find all of the possible peaceful states for these rooks. Problem StatementFor a board with size n x n, and n rooks. How many ways can n rooks be places s.t. they are peaceful? VisualizationLet us assume that we have some function Q, which defines as Q(n) &#x3D; number of configurations of n peaceful rooks in a (n x n) board. If we have a 1x1 board, then it is obvious that Q(1) &#x3D; 1. What if we have a 2x2 board? Easy. Here are the possible configurations, resulting in Q(2) &#x3D; 2. 3x3 board. Q(3) &#x3D; 4 4x4 board. Q(4) &#x3D; 10 ExamineWe don’t have a pattern yet, but we can assume that there is likely a pattern going on here. For instance, you might recognize similar rook layouts from the 2x2 board that appears on the 3x3 board or 4x4 board. This is no coincidence, there IS a pattern! We just need to decipher the problem further. The best way to start out is to look at the baseline, the 1x1 board. We know that Q(1) is the smallest board state we can have. Thus, we can say that Q(1) is our baseline. Then in a 2x2 board, there exists 4 total possible combinations of rooks in each position. However, only two of them have peaceful rooks, so we can include Q(2) &#x3D; 2 as another base case. The reason why I am including these two as base cases, would matter a bit more afterwards, but since we already know the answer for Q(1) and Q(2), might as well set those to true. In a 3x3 board, there is a total of 9 different positions with various combinations of peaceful rooks. For instance, one possible placement is when all of the rooks are diagonal to each other. Here is an example Let us visualize the bottom left rook. The red arrows display all the possible positions where this rook can attack. Thus, any other position that we place the other two rooks can only be inside of the green square. If you look at the green square, we can see that this configuration is similar to our 2x2 board congiruation! Even if we use the other pattern it is valid. Let’s call this rook the “pivot rook”. What if we move the bottom left root up one space? Then, the rook in the middle must move away from the center to the bottom s.t. the rooks can be peaceful. So if the middle rook has to move down one space for this configuration, the only configurations allowed for the top right rook has to be inside of the green square (which is one). Finally, if we move the rook up one more step, then we have to move the rook in the bottom middle up to the middle and the room to the top right all the way down. All in all, just based of from one rook moving, we can find all of the configurations for this 3x3 board. Why is that so? If we take any other rook and move it likewise, it will create the same pattern we did with the bottom right rook we moved. Furthermore, if we look at the total amount of configurations, especially by examining the green squares, we can add up the amount of total peaceful rooks as: Q(3) &#x3D; Q(2) + Q(1) + Q(1) &#x3D; 4 We know that Q(3) &#x3D; 4, and this is correct! Is this another coincidence? Well… let’s find out. Here is a larger example with more visualization. A 4x4 board. Lets start at some diagonal position, and take the bottom left rook as the pivot rook. At this position, ignoring the red squares, we can make a smaller 3x3 board which yields four different arragements. Then, we can move the pivot rook up, here you can see that we created a 2x2 board. If we move the pivot rook up again, we get a weird configuration like this: Although it does not seem like we can use the configuration from another board, there is a hidden board here! If you take all of the squares and then removed all of the squares with the color blue and red, you will get a board with an island of different squares. We can combine these board, and create a pseudo-board. This is practically a replication of a n x n board due to however many squares are left. Moreover, since we are not using the squares that are being attacked by the other rooks, we can guarantee that these squares are different combinations. Finally, we move the pivot rook up again, which will create the configuration here. Combining all of the results, we get Q(4) &#x3D; 4 + 2 + 2 + 2 &#x3D; 10. Do you see the pattern now? Since we move the pivot rook 4 times, we add 4 times, based on the total amount of confiurations that we can have. Then, we can say Q(4) is equivalent to Q(4) &#x3D; Q(3) + Q(2) + Q(2) + Q(2). That means if we start with a diagonal, and using the property of the pivot rook moving upwards, we can guarantee that the first pattern uses the peacful board combinations from Q(n - 1). Then for each other term that we add up, it will be Q(n - 2) since we have to move another rook when we move the pivot rook. Finally, we can come up with this: Q(n) &#x3D; Q(n - 1) + Q(n - 2) * (n - 1) SolutionLet Q(n) &#x3D; number of configurations of n peaceful rooks in a n x n board. Q(n) &#x3D; Q(n - 1) + Q(n - 2) * (n - 1), where Q(1) &#x3D; 1 and Q(2) &#x3D; 2. Such an elegant solution. CodeNote that we do Q[i] &#x3D; Q[i - 1] + Q[i - 2] * (i) since i is 0th indexed. 123456789101112def solution(n): if n == 1 or n == 2: return n Q = n * [0] Q[0] = 1 Q[1] = 2 for i in range(2, n): Q[i] = Q[i - 1] + Q[i - 2] * (i) return Q[n - 1] print(solution(3)) ———- 4print(solution(4)) ———- 10print(solution(8)) ———- 764print(solution(50)) ———- 27886995605342342839104615869259776 If you’d like to check my work or want me to write a proper proof, please do a 50 x 50 chess board on paper. EndThank you for reading and have a good day.","tags":["math","python","computer science","problem solving","chess"],"categories":["Chess"]},{"title":"L2-regularized Logistic Regression Derived","path":"/2024/07/21/l2-regularized-lr-math/","content":"L2-regularized Logistic RegressionLogistic regression is a very popular algorithm as it is fast to use and generally reliable for regression problems. L2 regularization or ridge regression or more commonly known as weight decay, is a technique that modifies a loss function by adding a regularization term to penalize drastic changes in the weight fitting well with the data. Here, I provide the derivation for l2-regularized logistic regression In order to derive the loss for logistic regression, we need to compute the gradient and hessians of that loss function. First we can define the function as: $$ f(ω’) &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^nlog(1 + exp(-y_i(ω_0 + ω^Tx_i))) + λ ||ω||_2^2 $$ Furthermore, note that the bias is not regularized; therefore, we consider $ω_0 &#x3D; [ω_0; ω]$ as the vertical concatenation of the bias and feature coefficients. Before we find the gradient, it is imperative for us to break down the function to help. Moreover, let us define another separate function that will come into use later. The Sigmoid Function ($\\sigma$)The sigmoid function, which is generally represented as $\\sigma$, is $$\\text{sigmoid}(x) &#x3D; \\frac{e^x}{1 + e^x} &#x3D; \\frac{exp(x)}{1 + exp(x)} &#x3D; \\sigma(x)$$ visualized sigmoid function Prior to it’s now common use for machine learning as an “activation function”, in a general sense, it is just a function that maps a value (x) to another value between 0 and 1. In a binary problem, we see this as an arbitrary number that evaluates to either true or false according to the input value. Moreover, let us find the derivative of the sigmoid function (hint: we will need it later) I’m too lazy to add the derivation for this. here it is.$$ \\sigma’(x) &#x3D; \\sigma(x)(1-\\sigma(x)) $$ blue line is the derivative of the sigmoid function Computing for $∇_{w’}f$In order to find the gradient, we need to find the derivative of the function according to the bias and feature coefficients. The order does not matter much. $$ ∇_{w’}f &#x3D; [\\frac{\\partial f}{\\partial w_0}; \\frac{\\partial f}{\\partial w}] $$ Finding $\\frac{\\partial f}{\\partial w_0}$Here we apply $\\frac{\\partial}{\\partial w_0}$ and separate for simplicity $$ (\\frac{\\partial}{\\partial w_0})(\\frac{1}{n} \\sum_{i&#x3D;1}^n \\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) + λ ||w||_2^2) $$ we can ignore the summation since it applies to all n, it wont change the derivative (we will have to substitute it back later) $$ (\\frac{\\partial}{\\partial w_0})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) + ((\\frac{\\partial}{\\partial w_0})λ||w||_2^2) $$ we can separate the into two parts to make it easier to derive overall Derive $(\\frac{\\partial}{\\partial w_0})λ||w||_2^2$: $$ (\\frac{\\partial}{\\partial w_0})λ||w||_2^2 &#x3D; 0 $$ Derive $(\\frac{\\partial}{\\partial w_0})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i)))$: $$ (\\frac{\\partial}{\\partial w_0})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) $$apply the derivative due to log$$ \\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * \\frac{\\partial}{\\partial w_0} (1 + \\exp(-y_i(ω_0 + w^Tx_i))) $$apply the derivative again due to the exponent$$\\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * (\\exp(-y_i(ω_0 + w^Tx_i)) * (\\frac{\\partial}{\\partial w_0} * -y_iω_0 - y_iw^Tx_i)) $$apply the derivative, we can remove the term on the right since it is $w^T$$$\\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * \\exp(-y_i(ω_0 + w^Tx_i)) * -y_i $$combine the terms together$$-y_i * \\frac{\\exp(-y_i(ω_0 + w^Tx_i))}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} $$using the sigmoid function we defined earlier, we can dress up the function$$ -y_i * \\frac{\\exp(-y_i(ω_0 + w^Tx_i))}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} \\rightarrow -y_i *\\sigma(-y_i(w_0 + w^Tx_i))$$ Then place all derived values back into $\\frac{\\partial f}{\\partial w_0}$, $$ \\frac{1}{n} \\sum_{i&#x3D;1}^n ((\\frac{\\partial}{\\partial w_0})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i)))) + ((\\frac{\\partial}{\\partial w_0})λ||w||_2^2) $$ $$ \\frac{1}{n} \\sum_{i&#x3D;1}^n -y_i *\\sigma(-y_i(w_0 + w^Tx_i)) $$ $$ \\frac{\\partial f}{\\partial w_0} &#x3D; - \\frac{1}{n} \\sum_{i&#x3D;1}^n y_i *\\sigma(-y_i(w_0 + w^Tx_i)) $$ Finding $\\frac{\\partial f}{\\partial w}$:follows the same format as before $$(\\frac{\\partial}{\\partial w})(\\frac{1}{n} \\sum_{i&#x3D;1}^n \\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) + λ ||w||_2^2)$$ $$\\frac{1}{n} \\sum_{i&#x3D;1}^n ((\\frac{\\partial}{\\partial w})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) + ((\\frac{\\partial}{\\partial w})λ||w||_2^2) $$ Derive $(\\frac{\\partial}{\\partial w})λ||w||_2^2$: $$ (\\frac{\\partial}{\\partial w_0})λ||w||_2^2 &#x3D; 2\\lambda w $$ Derive $(\\frac{\\partial}{\\partial w})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i)))$: $$ (\\frac{\\partial}{\\partial w})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) $$apply the derivative due to log$$ \\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * \\frac{\\partial}{\\partial w} (1 + \\exp(-y_i(ω_0 + w^Tx_i))) $$apply the derivative again due to the exponent$$ \\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * \\exp(-y_i(ω_0 + w^Tx_i)) * (\\frac{\\partial}{\\partial w} * -y_iω_0 - y_iw^Tx_i)) $$apply the derivative, and we dont eliminate the right term this time$$ \\frac{1}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} * \\exp(-y_i(ω_0 + w^Tx_i)) * -y_ix_i $$organize the function$$ -y_ix_i * \\frac{\\exp(-y_i(ω_0 + w^Tx_i))}{1 + \\exp(-y_i(ω_0 + w^Tx_i))}$$using the sigmoid function,$$ -y_ix_i * \\frac{\\exp(-y_i(ω_0 + w^Tx_i))}{1 + \\exp(-y_i(ω_0 + w^Tx_i))} \\rightarrow -y_ix_i *\\sigma(-y_i(w_0 + w^Tx_i))$$ Then place all derived values back into $\\frac{\\partial f}{\\partial w}$, $$ \\frac{1}{n} \\sum_{i&#x3D;1}^n ((\\frac{\\partial}{\\partial w})\\log(1 + \\exp(-y_i(ω_0 + w^Tx_i))) + ((\\frac{\\partial}{\\partial w})λ||w||_2^2)) $$ $$\\frac{1}{n} \\sum_{i&#x3D;1}^n -y_i *\\sigma(-y_i(w_0 + w^Tx_i)) + 2\\lambda w $$ $$\\frac{\\partial f}{\\partial w} &#x3D; - \\frac{1}{n} \\sum_{i&#x3D;1}^n y_ix_i *\\sigma(-y_i(w_0 + w^Tx_i)) + 2\\lambda w $$ Getting $∇_{w’}f$ $$∇_{w’}f &#x3D; [\\frac{\\partial f}{\\partial w_0}; \\frac{\\partial f}{\\partial w}]$$$$ &#x3D; [ - \\frac{1}{n} \\sum_{i&#x3D;1}^n y_i *\\sigma(-y_i(w_0 + w^Tx_i)) ; - \\frac{1}{n} \\sum_{i&#x3D;1}^n y_ix_i *\\sigma(-y_i(w_0 + w^Tx_i))+ 2\\lambda w] $$ we can combine these terms together to form: A:$$∇_{w’}f&#x3D; - \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i *\\sigma(-y_i(w_0 + w^Tx_i)) * \\begin{bmatrix} 1\\\\ x_i \\end{bmatrix}) + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$ Computing for $ ∇_{w’}^2f$in order to compute the hessian, we can take a shortcut and find the derivative based on each part of the gradient according to the hessian matrix. $$ ∇_{w’}^2f &#x3D; \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial w_0^2} \\frac{\\partial^2 f}{\\partial w_0 \\partial w} \\\\ \\frac{\\partial^2 f}{\\partial w \\partial w_0} \\frac{\\partial^2 f}{\\partial w^2} \\end{bmatrix}$$ we can say that $\\frac{\\partial^2 f}{\\partial w_0 \\partial w} &#x3D; \\frac{\\partial^2 f}{\\partial w \\partial w_0}$ due to the rule of derivatives For the regularization term, the only time we add $2\\lambda$ is when the we’re taking $\\frac{\\partial^2 f}{\\partial w^2}$. Therefore, in our matrix, we can just represent the 2nd row and 2nd column ($\\frac{\\partial^2 f}{\\partial w^2}$) for the regularization term. That means we just set the regularization term as $ + \\begin{bmatrix} 0 &amp; 0 \\\\ 0 &amp; 2\\lambda \\end{bmatrix}$. Using chain rule for $\\sigma(-y_i(w_0 + w^Tx_i))$ $ abla_{w_0}$: $$ \\sigma’(-y_i(w_0 + w^Tx_i)) &#x3D; \\sigma’ * (-y_i(w_0 + w^Tx_i))’\\&#x3D; -y_i * \\sigma(1 - \\sigma)$$ $ abla_w$: $$ \\sigma’(-y_i(w_0 + w^Tx_i)) &#x3D; \\sigma’ * (-y_i(w_0 + w^Tx_i))’\\&#x3D; x_i * \\sigma(1 - \\sigma)$$ Then we can say for the hessian matrix, we can replace the result of what the partial derivative is for each respective $w$ or $w_0$ $$ ∇_{w’}^2f &#x3D; \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial w_0^2} \\frac{\\partial^2 f}{\\partial w_0 \\partial w} \\\\ \\frac{\\partial^2 f}{\\partial w \\partial w_0} \\frac{\\partial^2 f}{\\partial w^2} \\end{bmatrix}$$ For $\\frac{\\partial^2 f}{\\partial w_0^2}$: $-y_i * \\sigma(1 - \\sigma)$ since it is $\\frac{\\partial f}{\\partial w_0}$ $$ &#x3D; - \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i * -y_i * \\sigma(1 - \\sigma) * \\begin{bmatrix} 1\\\\ x_i \\end{bmatrix}) + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$ $$ &#x3D; \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i^2 * \\sigma(1 - \\sigma)) * 1 + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$ For $\\frac{\\partial^2 f}{\\partial w^2}$: $x_i * \\sigma(1 - \\sigma)$ since it is $\\frac{\\partial f}{\\partial w_0}$ $$ &#x3D; - \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i * x_i * \\sigma(1 - \\sigma) * \\begin{bmatrix} 1\\\\ x_i \\end{bmatrix}) + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix}$$ $$ &#x3D; \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i^2 * \\sigma(1 - \\sigma)) * x_i + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$ For $\\frac{\\partial^2 f}{\\partial w \\partial w_0} &#x3D; \\frac{\\partial^2 f}{\\partial w_0 \\partial w}$ $x_i * \\sigma(1 - \\sigma)$ since it is $\\frac{\\partial f}{\\partial w_0}$ $$ &#x3D; - \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i * \\sigma(1 - \\sigma) * \\begin{bmatrix} 1\\\\ x_i \\end{bmatrix}) + 2\\lambda $$$$ &#x3D; - \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i * \\sigma(1 - \\sigma) * x_i) + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$or$$ &#x3D; \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i * x_i * \\sigma(1 - \\sigma)) * 1 + 2\\lambda \\begin{bmatrix} 0\\\\ w \\end{bmatrix} $$ We can put all of the options from the gradients due to the hessian matrix (representing $x_i$): $$ \\begin{bmatrix} \\frac{\\partial^2 f}{\\partial w_0^2} \\frac{\\partial^2 f}{\\partial w_0 \\partial w} \\\\ \\frac{\\partial^2 f}{\\partial w \\partial w_0} \\frac{\\partial^2 f}{\\partial w^2} \\end{bmatrix} &#x3D; \\begin{bmatrix} 1 &amp; x_i\\\\ x_i &amp; x_i^2 \\end{bmatrix} $$ Then we can put it together and form: A:$$ ∇_{w’}^2f &#x3D; \\frac{1}{n} (\\sum_{i&#x3D;1}^n y_i \\sigma(-y_i(w_0 + w^Tx_i))(1 - \\sigma(-y_i(w_0 + w^Tx_i))) * \\begin{bmatrix} 1 &amp; x_i\\\\ x_i &amp; x_i^2 \\end{bmatrix}) + \\begin{bmatrix} 0 &amp; 0\\\\ 0 &amp; 2\\lambda \\end{bmatrix} $$ L2-Regularized Multinomial Logistic RegressionFor an extra challenge, here we derive the gradients for the loss function of a l2-regularized multinomial logistic regression $$f(W, b) &#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n[y_i^T(Wx_i + b) - log(1^Texp(Wx_i + b))] + \\lambda||W||_F^2$$ Softmax(To be updated) Let us represent the softmax function: $$\\text{softmax}(x) &#x3D; \\frac{exp(x)}{1^T exp(x)}$$ Computing for fFinding $ abla_W f$: $$ f(W, b) &#x3D; $$$$ -\\frac{1}{n}\\sum_{i&#x3D;1}^n[( abla_W)y_i^T(Wx_i + b) - ( abla_W)log(1^Texp(Wx_i + b))] + ( abla_W) \\lambda||W||_F^2$$ Derive $ abla_W λ||W||_F^2$: $$ &#x3D; 2\\lambda W $$ Derive $ abla_W y_i^T(Wx_i + b)$: $$ &#x3D; y_i^Tx_i $$ Derive $ abla_W log(1^Texp(Wx_i + b))$: using the chain rule twice:$$ \\log’(1^T exp(Wx_i + b)) * \\sum_{l &#x3D; 1}^K abla_W [exp(Wx_i + b)_l] $$ apply the log $$ \\frac{1}{1^T exp(Wx_i + b)} * \\sum_{l &#x3D; 1}^K [exp(Wx_i + b)_l] * abla_W [&lt;w_l, x_i&gt;] $$ note that $ abla_W [&lt;w_l, x_i&gt;]$ is equivalent to $x_i$ since we are taking the cross product of a static $x_i$ given for all ls in the sum, and then the gradient of such would result in $x_i$ $$ &#x3D; \\frac{1}{1^T exp(Wx_i + b)} * \\sum_{l &#x3D; 1}^K [exp(Wx_i + b)_l] * x_i$$ combine $$ &#x3D; \\sum_{l &#x3D; 1}^K\\frac{[exp(Wx_i + b)_l]}{1^T exp(Wx_i + b)} * x_i$$ use softmax $$ &#x3D; \\sum_{l &#x3D; 1}^K softmax(Wx_i + b)_l * x_i$$ $$ &#x3D; softmax(Wx_i + b) * x_i$$ plug in for the main equation $$ abla_W f(W, b) &#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n[y_i^Tx_i - softmax(Wx_i + b) * x_i] + 2\\lambda W$$$$&#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n(y_i^T - softmax(Wx_i + b)) * x_i + 2\\lambda W $$ Finding $ abla_b f$: Derive $ abla_b λ||W||_F^2$: $$ &#x3D; 0$$ Derive $ abla_b y_i^T(Wx_i + b)$: $$ &#x3D; y_i$$ Derive $ abla_b log(1^Texp(Wx_i + b))$: using the chain rule twice:$$ &#x3D; \\log’(1^T exp(Wx_i + b)) * \\sum_{l &#x3D; 1}^K abla_W [exp(Wx_i + b)_l] $$ apply the derivative again due to the exponent $$ \\frac{1}{1^T exp(Wx_i + b)} * \\sum_{l &#x3D; 1}^K * [exp(Wx_i + b)_l] * abla_W [&lt;w_l, x_i&gt;] $$since we are finding the gradient in respect to b, we can cancel out terms:$$ \\frac{1}{1^T exp(Wx_i + b)} * [exp(Wx_i + b)_l] $$ add the summation back $$ \\sum_{l &#x3D; 1}^K \\frac{exp(Wx_i + b)_l}{1^T exp(Wx_i + b)} $$ use softmax $$ \\sum_{l &#x3D; 1}^K softmax(Wx_i + b)_l $$$$ softmax(Wx_i + b) $$ Put it all together $$ abla_b f(W, b) &#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n y_i - softmax(Wx_i + b) $$ Both results$∇_Wf$$$ &#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n(y_i - softmax(Wx_i + b))x_i^T + 2\\lambda W $$ $∇_bf$$$ &#x3D; -\\frac{1}{n}\\sum_{i&#x3D;1}^n(y_i - softmax(Wx_i + b)) $$ Empirically verify our derivation utilizing PyTorchHere we will use PyTorch as a tool to evaluate the gradient, then we will apply our own solution to get a result. Load and prepare a dataset Using sklearn wine dataset and one hot encode the targets 123456789101112131415161718192021222324252627282930313233343536from sklearn.datasets import load_winefrom sklearn.preprocessing import OneHotEncoderwine_data = load_wine()X_wine = wine_data.dataY_wine = wine_data.target# add to Y dimY_wine = Y_wine.reshape(-1, 1)enc = OneHotEncoder(sparse_output=False)enc.fit(Y_wine)Y_onehot = enc.transform(Y_wine)n, d = X_wine.shapen, k = Y_onehot.shape# convert to pytorchY_onehot = torch.from_numpy(Y_onehot).to(torch.float)X_wine = torch.from_numpy(X_wine).to(torch.float)# init parameter tensor (w) with bias (w0 = 1)def init_weights_wine(d, k): w = torch.ones((d, k), device=device, requires_grad=True) w = w/(len(X_wine)) # prevent overflow from exp w.retain_grad() return wdef init_bias_wine(k): b = torch.ones((1, k), device=device, requires_grad=True) b = b/(len(X_wine)) # prevent overflow from exp b.retain_grad() return bW_test = init_weights_wine(d, k)b_test = init_bias_wine(k) Define the loss function for MLRDefine the function so that we can call .backward() from pytorch and evaluate it from our answer. Loss function 123# the loss function for l2 regularized multinomial logistic regressiondef f_multi(X, Y, W, b, lambda_v=0): return -torch.mean(torch.sum((Y * torch.matmul(X, W) + b), dim=1) - torch.logsumexp(torch.matmul(X, W) + b, dim=1)) + lambda_v * torch.norm(W, &#x27;fro&#x27;)**2 Prepare and compare both our solution and pytorch Evaluate the gradient using pytorch 12345678W_pytorch = init_weights_wine(d, k)b_pytorch = init_bias_wine(k)W_pytorch.grad = Noneb_pytorch.grad = Noneloss = f_multi(X_wine, Y_onehot, W_pytorch, b_pytorch, lambda_v=1)loss.backward() Define and evaluate our derivation 12345678910# the loss function for l2 regularized multinomial logistic regressiondef f_multi_grad_w(X, Y, W, b, lambda_v = 0): logits = torch.matmul(X, W) + b return -torch.matmul((Y - torch.softmax(logits, dim=1)).T, X) / len(X) + 2 * lambda_v * Wmy_W = init_weights_wine(d, k)my_b = init_bias_wine(k)myWGrad = f_multi_grad_w(X_wine, Y_onehot, my_W, my_b, lambda_v=1)myBGrad = f_multi_grad_b(X_wine, Y_onehot, my_W, my_b) Empirical Results Weight gradients 1torch.norm(W_pytorch.grad, &#x27;fro&#x27;) - torch.norm(myWGrad, &#x27;fro&#x27;) tensor(4.5776e-05, grad_fn&#x3D;) Bias gradients 1torch.norm(W_pytorch.grad, &#x27;fro&#x27;) - torch.norm(myWGrad, &#x27;fro&#x27;) tensor(-2.9802e-08, grad_fn&#x3D;)","tags":["math","machine-learning","python","pytorch"],"categories":["Machine Learning"]}]